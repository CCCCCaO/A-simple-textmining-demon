{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 文本预处理与聚类分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "[顾思成](https://github.com/CCCCCaO) 2019.5.27\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 概述\n",
    "\n",
    "爬虫是一种**数据搜集**手段，可以获取到文本，图像，音频，乃至视频等诸多数据。\n",
    "而仅仅爬取到了数据是没有用的，在此基础上，我们还要对数据进行分析与解读，让数据为我所用。\n",
    "\n",
    "文本挖掘的一般步骤：\n",
    "数据搜集->文本预处理->数据挖掘与可视化->模型搭建->模型评估\n",
    "\n",
    "在这里简单介绍一下**文本数据预处理与聚类分析**。个人拙见，还有待学习提高！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 运行环境\n",
    "\n",
    "+ Win10 64bit\n",
    "+ Anaconda 3.7\n",
    "+ Jupyter Notebook\n",
    "+ Jieba\n",
    "+ Scikit-learn\n",
    "+ Plotly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 1、中文分词\n",
    "文本预处理的第一步，也是后续各种分析处理的基础，即分词处理。\n",
    "\n",
    "词是人们理解语言的基础单位之一，就和我们学英语一样，你要理解某一句子，必然是建立在你理解其中大部分单词含义的基础之上的。\n",
    "\n",
    "那么为了让计算机理解，我们也模仿人类的认识过程，也将文本切成一个个的词语，提取其中特征，来帮助后续处理。\n",
    "\n",
    "这里采用了[Jieba中文分词组件](https://github.com/fxsjy/jieba)，另外有其他的分词组件如清华的Thulac等可选择使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 使用Jieba中文分词示例\n",
    "seg_list = jieba.cut(\"我来到上海理工大学\", cut_all=True)\n",
    "print(\"【全模式】: \" + \"/ \".join(seg_list))         # 全模式 即完完全全地切分\n",
    "\n",
    "seg_list = jieba.cut(\"我来到上海理工大学\", cut_all=False)\n",
    "print(\"【精确模式】: \" + \"/ \".join(seg_list))       # 精确模式 将一些应该属于一个词汇的东西合并\n",
    "\n",
    "seg_list = jieba.cut(\"他来到了新媒体技术专业的学院\")   # 对于未收录在语料库种的新词进行识别预测\n",
    "print(\"【新词识别】: \" + \"/ \".join(seg_list))\n",
    "\n",
    "seg_list = jieba.cut_for_search(\"小明本科毕业于上海理工大学，后在中科院计算所深造\")  # 搜索引擎模式\n",
    "print(\"【搜索引擎模式】: \" + \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "我们现在假设有这么七个文本,我们试一下进行切分\n",
    "1. 文本向量化是一种提取特征的操作\n",
    "2. 分析文本应该关注文本的特征\n",
    "3. 床前明月光\n",
    "4. 苹果是水果中的一种,深受人们喜爱\n",
    "5. 今年下半年中美合拍的西游记\n",
    "6. 我最爱吃苹果\n",
    "7. 苹果是我的最爱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "corpus=[\"文本向量化是一种提取特征的操作\",\n",
    "       \"分析文本应该关注文本的特征\",\n",
    "       \"床前明月光\",\n",
    "        \"苹果是水果中的一种深受人们的喜爱\",\n",
    "       \"今年下半年中美合拍的西游记\",\n",
    "       \"我最爱吃苹果\",\n",
    "       \"苹果是我的最爱\"]\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    seg_list = jieba.cut(corpus[i], cut_all=False)\n",
    "    print(i+1, \": \", \"/ \".join(seg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 2、文本特征处理\n",
    "现在我们已将文本切分成一堆的词组成的东西，但是对于计算机而言还是无法理解。我们要对这些词特征，再进行特征提取，将词汇以及其中的联系关系转换成数值，这样计算机才能够认识它。特征提取有向量化与Hash Trick等，而向量化又是最为常用的。\n",
    "\n",
    "文本向量化即是将文本变成一个向量，好比用某个坐标点来标记某个文本。文本向量化的方法有很多，例如最简单的词频统计，按照某个词在这篇文章中出现的频率作为这篇文章的特征向量。而这里我们用TF-IDF值。TF-IDF值是对词频的一种修正改善，减小了某些出现次数很多但又没有意义的词如“的”，“了”等词的影响。\n",
    "推荐阅读[文本挖掘之TF-IDF值](https://www.cnblogs.com/pinard/p/6693230.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 示例文本\n",
    "corpus=[\"文本 向 量化 是 一种 提取 特征 的 操作\",\n",
    "       \"分析 文本 应该 关注 文本 的 特征\",\n",
    "       \"床 前 明月光\",\n",
    "        \"苹果 是 水果 中 的 一种 深受 人们 的 喜爱\",\n",
    "       \"今年 下半年 中 美 合拍 的 西游记\",\n",
    "       \"我 最 爱 吃 苹果\",\n",
    "       \"苹果 是 我 的 最爱\",]\n",
    "tfidf = TfidfVectorizer()                                      # 生成TFIDF对象\n",
    "re = tfidf.fit_transform(corpus)                               # 用TFIDF拟合并标准化\n",
    "\n",
    "# 结果输出\n",
    "wordlist = tfidf.get_feature_names()                           #获取词袋模型中的所有词  \n",
    "weightlist = re.toarray()                                      # TF-IDF的矩阵 元素re[i][j]表示j词在i类文本中的TF-IDF权重\n",
    "for i in range(len(weightlist)):  \n",
    "    print (\"-------第\",i+1,\"段文本的词语tf-idf权重------\")  \n",
    "    for j in range(len(wordlist)):  \n",
    "        print (wordlist[j],weightlist[i][j])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(weightlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 3、聚类分析\n",
    "\n",
    "对于我们而言，是一句或一段话，而现在，经过上面的向量化，对于计算机而言，其实就是一串数字组成的数据，即一个矩阵。那么这些数据之间有什么联系？如何去分析呢？那么就需要对文本建立数学模型，进行解读与分析。方法有很多种：文本摘要，关键词抽取，文本聚类，文本分类，文档主题模型，观点抽取，情感分析等等。\n",
    "\n",
    "人类在研究事物的过程中，常将东西分门别类来观察研究。而将物理或抽象对象的集合分成由类似的对象组成的多个类的过程被称为聚类，最直观理解即是：“物以类聚，人以群分”。通过聚类分析我们可以使得数据变量中的一些关联性的特征呈现出来，找到数据之间的相似性，相似的放在一类里面。\n",
    "\n",
    "需要注意的是分类和聚类是不一样的，\n",
    ">最直白的理解，聚类就是给你一堆东西，然后分成几类。分类就是给你几类东西，再给你一个。然后这一个属于哪一类。 --知乎的Tavion Fu\n",
    "\n",
    "聚类的方法有很多种，DBSCAN,DPEAK等等，这里介绍一下K-means聚类。\n",
    "\n",
    "K-means聚类算法的过程：\n",
    "\n",
    "1. 在所有数据中随机选取K个中心点\n",
    "2. 分别计算每个点到每个中心点的距离，对于每个点来说选取距离其最近的那个中心点，归为一类。\n",
    "3. 重新计算K个中心点，即对该类所有点的坐标求均值means，得到新的中心点\n",
    "4. 重复第二步，直至中心点变化极小为止\n",
    "\n",
    "推荐阅读[K-means聚类算法 JerryLead博客](https://www.cnblogs.com/jerrylead/archive/2011/04/06/2006910.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 使用Kmeans聚类 假设有4类\n",
    "n_clusters = 4\n",
    "kmeans = KMeans(n_clusters)\n",
    "kmeans.fit(weightlist) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "对于下面这7个文本而言\n",
    "1. 文本向量化是一种提取特征的操作\n",
    "2. 分析文本应该关注文本的特征\n",
    "3. 床前明月光\n",
    "4. 苹果是水果中的一种,深受人们喜爱\n",
    "5. 今年下半年中美合拍的西游记\n",
    "6. 我最爱吃苹果\n",
    "7. 苹果是我的最爱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 结果输出\n",
    "label_all = []\n",
    "for index, label in enumerate(kmeans.labels_, 1):\n",
    "    label_all.append(label)\n",
    "    print(\"文本序号 {} , 是第 {} 类\".format(index, label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## 4、可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "其实我们已经得到分类的结果了，如上面输出的那样。可是这不直观，还需要你去上下结合着看，当文本数量激增的时候，非常头疼，这时候我们需要进行可视化的操作，使得结果更浅显易懂，直观生动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "py.init_notebook_mode(connected=False)\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 使用T-SNE进行降维 将高维空间的TF-IDF矩阵降低到2维平面 并标准化\n",
    "tsne = TSNE(n_components=2, init='random')                   \n",
    "decomposition_data = tsne.fit_transform(weightlist)     \n",
    "X = StandardScaler().fit_transform(decomposition_data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 使用Plotly进行可视化操作\n",
    "text = [index for index, label in enumerate(kmeans.labels_, 1)]\n",
    "\n",
    "trace = [go.Scatter(\n",
    "        x=X[:, 0], \n",
    "        y=X[:, 1],\n",
    "        text=text,\n",
    "        textposition='bottom center', \n",
    "        marker=dict(\n",
    "            symbol=\"circle\",\n",
    "            size = 10,\n",
    "            color=kmeans.labels_,  # 每个点的颜色是它聚类后的类别\n",
    "            opacity=0.8,\n",
    "            colorscale='Jet',\n",
    "            showscale=True\n",
    "        ),\n",
    "        mode='markers'\n",
    "    )]\n",
    "title = '7篇文本的K-Means聚类 K=' + str(n_clusters)\n",
    "layout = go.Layout(title=title) \n",
    "fig = go.Figure(data=trace, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# 生成一组数据\n",
    "n_samples = 1500\n",
    "random_state = 170\n",
    "X_varied, y_varied = make_blobs(n_samples=n_samples,\n",
    "                                cluster_std=[1.0, 2.5, 0.5],\n",
    "                                random_state=random_state)\n",
    "# Kmeans聚类\n",
    "n_clusters = 5\n",
    "y_pred = KMeans(n_clusters, random_state=random_state)\n",
    "y_pred.fit_predict(X_varied)\n",
    "\n",
    "# 绘图\n",
    "trace = [go.Scatter(\n",
    "        x=X_varied[:, 0], \n",
    "        y=X_varied[:, 1],\n",
    "        textposition='bottom center', \n",
    "        marker=dict(\n",
    "            symbol=\"circle\",\n",
    "            size = 10,\n",
    "            color=y_pred.labels_,\n",
    "            opacity=0.8,\n",
    "            colorscale='Jet',\n",
    "            showscale=True\n",
    "        ),\n",
    "        mode='markers'\n",
    "    )]\n",
    "title = 'K-Means聚类示例 K=' + str(n_clusters)\n",
    "layout = go.Layout(title=title) \n",
    "fig = go.Figure(data=trace, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
